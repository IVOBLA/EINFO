# EINFO - Projekt-Struktur & Konfiguration

**Letzte Aktualisierung:** 2025-12-22
**Status:** âœ… Knowledge-System vollstÃ¤ndig indiziert (22/22 Dateien)

---

## ðŸ“ Ãœbersicht der Hauptordner

```
EINFO/
â”œâ”€â”€ server/              # Haupt-Backend (Express, Websockets)
â”œâ”€â”€ client/              # Frontend (React/Vue)
â”œâ”€â”€ chatbot/             # KI-Chatbot System (Llama 3.1)
â”œâ”€â”€ feldkirchen-adressen/
â””â”€â”€ script/              # Utilities
```

---

## ðŸ¤– CHATBOT - Detaillierte Struktur

Das Chatbot-System ist ein eigenstÃ¤ndiges Modul mit eigener Package.json.

### Hauptverzeichnisse

```
chatbot/
â”œâ”€â”€ server/                    # Backend-Logik
â”‚   â”œâ”€â”€ config.js              # âš™ï¸ HAUPT-KONFIGURATION
â”‚   â”œâ”€â”€ index.js               # Express-Server, API-Endpunkte
â”‚   â”œâ”€â”€ sim_loop.js            # Simulationsschleife
â”‚   â”œâ”€â”€ llm_client.js          # LLM-Kommunikation (Ollama)
â”‚   â”œâ”€â”€ prompts.js             # Prompt-Management
â”‚   â”œâ”€â”€ logger.js              # Logging-System
â”‚   â”œâ”€â”€ memory_manager.js      # Langzeit-Memory (RAG)
â”‚   â”œâ”€â”€ einfo_io.js            # Daten von/zu EINFO-Server
â”‚   â”œâ”€â”€ field_mapper.js        # Token-Optimierung (kurze Feldnamen)
â”‚   â”œâ”€â”€ simulation_helpers.js  # Rollen-Logik, Validierung
â”‚   â”œâ”€â”€ json_sanitizer.js      # LLM-Output Cleanup
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                   # ðŸ“š Knowledge-System (RAG)
â”‚   â”‚   â”œâ”€â”€ rag_vector.js      # Vector-Suche (Cosine-Similarity)
â”‚   â”‚   â”œâ”€â”€ index_builder.js   # ðŸ”¨ Index-Build-Script
â”‚   â”‚   â”œâ”€â”€ embedding.js       # Ollama Embedding-API
â”‚   â”‚   â””â”€â”€ chunk.js           # Text-Chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ prompt_templates/      # ðŸ“ Prompt-Templates (TXT)
â”‚   â”‚   â”œâ”€â”€ start_system_prompt.txt
â”‚   â”‚   â”œâ”€â”€ operations_system_prompt.txt
â”‚   â”‚   â”œâ”€â”€ operations_user_prompt.txt
â”‚   â”‚   â””â”€â”€ chat_system_prompt.txt
â”‚   â”‚
â”‚   â””â”€â”€ scenarios/             # ðŸŽ­ Ãœbungs-Szenarien
â”‚       â”œâ”€â”€ hochwasser_basic.json
â”‚       â”œâ”€â”€ hochwasser_feldkirchen.json
â”‚       â””â”€â”€ sturm_bezirk.json
â”‚
â”œâ”€â”€ knowledge/                 # ðŸ“š KNOWLEDGE-DATEIEN (22 Dateien)
â”‚   â”œâ”€â”€ e31.pdf               # Stabsarbeit-Richtlinie (948 KB)
â”‚   â”œâ”€â”€ richtlinie.pdf        # Feuerwehr-Richtlinie (1.7 MB)
â”‚   â”œâ”€â”€ E5_web.pdf            # Gefahren-EK (1.3 MB)
â”‚   â”œâ”€â”€ E6_compressed_web.pdf # Anforderungen (50 KB)
â”‚   â”‚
â”‚   â”œâ”€â”€ hochwasser.txt        # Hochwasser-Wissen
â”‚   â”œâ”€â”€ schnee.txt            # Schnee-Wissen
â”‚   â”œâ”€â”€ sturm.txt             # Sturm-Wissen
â”‚   â”œâ”€â”€ mure.txt              # Muren-Wissen
â”‚   â”œâ”€â”€ unfall.txt            # Unfall-Wissen
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_flood_hazards.json    # RAG fÃ¼r Hochwasser
â”‚   â”œâ”€â”€ rag_snow_hazards.json     # RAG fÃ¼r Schnee
â”‚   â”œâ”€â”€ rag_storm_hazards.json    # RAG fÃ¼r Sturm
â”‚   â”œâ”€â”€ rag_mudflow_hazards.json  # RAG fÃ¼r Muren
â”‚   â”œâ”€â”€ rag_accident_hazards.json # RAG fÃ¼r UnfÃ¤lle
â”‚   â”‚
â”‚   â”œâ”€â”€ rollen_Einsatzleiter.json    # Rolle: EL
â”‚   â”œâ”€â”€ rollen_LtStb.json            # Rolle: LtStb
â”‚   â”œâ”€â”€ rollen_S1_Personal.json      # Rolle: S1
â”‚   â”œâ”€â”€ rollen_S2_Lage.json          # Rolle: S2
â”‚   â”œâ”€â”€ rollen_S3_Einsatz.json       # Rolle: S3
â”‚   â”œâ”€â”€ rollen_S4_Versorgung.json    # Rolle: S4
â”‚   â”œâ”€â”€ rollen_S5_Kommunikation.json # Rolle: S5
â”‚   â””â”€â”€ rollen_S6_IT_Meldestelle.json # Rolle: S6
â”‚
â”œâ”€â”€ knowledge_index/           # ðŸ—‚ï¸ GENERIERTER INDEX (nicht editieren!)
â”‚   â”œâ”€â”€ meta.json              # Chunk-Metadaten (alle 22 Dateien)
â”‚   â”œâ”€â”€ embeddings.json        # Vektoren (ca. 320 Chunks)
â”‚   â””â”€â”€ index.json             # Legacy (leer)
â”‚
â”œâ”€â”€ logs/                      # ðŸ“Š LOG-DATEIEN
â”‚   â”œâ”€â”€ chatbot.log            # Allgemeine Logs
â”‚   â”œâ”€â”€ LLM.log                # LLM-Requests/-Responses
â”‚   â””â”€â”€ ops_verworfen.log      # Verworfene Operations
â”‚
â”œâ”€â”€ ingest/                    # ðŸ”§ Index-Build-Tools (Legacy)
â”‚   â””â”€â”€ ingest_all.js          # Alter Index-Builder
â”‚
â”œâ”€â”€ client/                    # ðŸ–¥ï¸ Web-GUI (Dashboard)
â”‚   â”œâ”€â”€ dashboard.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ package.json               # NPM-Konfiguration
â””â”€â”€ node_modules/              # Dependencies
```

---

## âš™ï¸ KONFIGURATION: chatbot/server/config.js

**Die zentrale Konfigurationsdatei fÃ¼r das gesamte Chatbot-System!**

### 1ï¸âƒ£ Verzeichnis-Pfade

```javascript
const base = {
  // Pfad zu EINFO-Server Daten (relativ zu chatbot/server/)
  dataDir: "../../server/data",

  // Knowledge & Index (relativ zu chatbot/server/rag/)
  knowledgeDir: "../../knowledge",
  knowledgeIndexDir: "../../knowledge_index",

  // ...
};
```

**Wichtig:**
- âœ… **RICHTIG:** `"../../knowledge"` (von server/rag/ aus 2 Ebenen hoch)
- âŒ **FALSCH:** `"../knowledge"` (zu wenig), `"../../chatbot/knowledge"` (zu viel)

**Warum diese Pfade?**
- Der Index-Builder lÃ¤uft von `chatbot/server/rag/index_builder.js`
- `__dirname` ist `/home/user/EINFO/chatbot/server/rag`
- `path.resolve(__dirname, "../../knowledge")` â†’ `/home/user/EINFO/chatbot/knowledge` âœ…

### 2ï¸âƒ£ LLM-Konfiguration

```javascript
const base = {
  // Ollama-Server URL
  llmBaseUrl: process.env.LLM_BASE_URL || "http://127.0.0.1:11434",

  // Models
  llmChatModel: process.env.LLM_CHAT_MODEL || "llama3.1:8b",
  llmEmbedModel: process.env.LLM_EMBED_MODEL || "mxbai-embed-large",

  // Timeouts
  llmChatTimeoutMs: 60000,      // 1 Min fÃ¼r Chat
  llmSimTimeoutMs: 300000,      // 5 Min fÃ¼r Simulation
  llmEmbedTimeoutMs: 30000,     // 30 Sek fÃ¼r Embeddings

  // Context-Window & Batch-Size
  llmNumCtx: 8192,              // Context-Tokens
  llmNumBatch: 512,             // Batch-Size

  // ...
};
```

**Umgebungsvariablen (optional):**
```bash
# Custom Ollama-URL
export LLM_BASE_URL=http://192.168.1.100:11434

# Anderes Model
export LLM_CHAT_MODEL=mistral:7b

# Debug-Modus
export CHATBOT_DEBUG=1
```

### 3ï¸âƒ£ RAG-Einstellungen

```javascript
const base = {
  rag: {
    dim: 1024,                     // Embedding-Dimension (mxbai-embed-large)
    indexMaxElements: 50000,       // Max. Chunks im Index
    topK: 5,                       // Top-K Chunks fÃ¼r Context
    maxContextChars: 2500,         // Max. Zeichen fÃ¼r LLM-Prompt
    scoreThreshold: 0.35           // Min. Similarity-Score
  },
  // ...
};
```

**Was bedeutet das?**
- `dim: 1024` â†’ mxbai-embed-large erzeugt 1024-dimensionale Vektoren
- `topK: 5` â†’ Es werden die 5 Ã¤hnlichsten Chunks gesucht
- `scoreThreshold: 0.35` â†’ Nur Chunks mit Similarity > 0.35 werden verwendet

**Tuning-Tipps:**
- **Mehr Context benÃ¶tigt?** â†’ `topK: 8`, `maxContextChars: 4000`
- **Schneller, weniger Kontext?** â†’ `topK: 3`, `maxContextChars: 1500`
- **Strengere Relevanz?** â†’ `scoreThreshold: 0.45`

### 4ï¸âƒ£ Prompt-Limits

```javascript
const base = {
  prompt: {
    maxBoardItems: 25,         // Max. Einsatzstellen im Prompt
    maxAufgabenItems: 50,      // Max. Aufgaben im Prompt
    maxProtokollItems: 30      // Max. ProtokolleintrÃ¤ge im Prompt
  },
  // ...
};
```

**Zweck:** Token-Limit einhalten (8192 Context-Tokens)

**Bei Token-Problemen:**
```javascript
// Reduziere Limits:
maxBoardItems: 15,
maxAufgabenItems: 30,
maxProtokollItems: 20
```

### 5ï¸âƒ£ Memory-RAG

```javascript
const base = {
  memoryRag: {
    longScenarioMinItems: 100,        // Min. EintrÃ¤ge fÃ¼r "lange Ãœbung"
    maxAgeMinutes: 720,               // Max. Alter: 12 Stunden
    recencyHalfLifeMinutes: 120,      // Halbwertszeit: 2 Stunden
    longScenarioTopK: 12              // Top-K fÃ¼r lange Ãœbungen
  },
  // ...
};
```

**Zweck:** Chatbot merkt sich wichtige Entscheidungen Ã¼ber die Zeit

### 6ï¸âƒ£ Profile (optional)

```javascript
const profiles = {
  default: {
    // Standard-Profil (base-Werte)
  },

  llama_8b_gpu: {
    // Optimiert fÃ¼r Llama 3.1 8B auf GPU
    llmChatModel: "llama3.1:8b",
    defaultTemperature: 0.25,
    rag: { topK: 5, maxContextChars: 2500 }
  },

  mixtral_gpu: {
    // Legacy: Mixtral
    llmChatModel: "mixtral_einfo",
    rag: { dim: 768, topK: 8 }
  }
};
```

**Verwendung:**
```bash
# Profil aktivieren
export CHATBOT_PROFILE=llama_8b_gpu

# Standard-Profil
unset CHATBOT_PROFILE
```

---

## ðŸ“‚ SERVER (EINFO-Haupt-Backend)

```
server/
â”œâ”€â”€ index.js              # Express-Server
â”œâ”€â”€ package.json          # Dependencies
â”‚
â”œâ”€â”€ routes/               # API-Routen
â”‚   â””â”€â”€ data/             # Datei-basierte Routes
â”‚
â”œâ”€â”€ data/                 # ðŸ—„ï¸ DATENBANK (JSON-Dateien)
â”‚   â”œâ”€â”€ board.json        # Einsatzstellen-Board
â”‚   â”œâ”€â”€ protocol.json     # Protokoll
â”‚   â”œâ”€â”€ roles.json        # Aktive Rollen
â”‚   â”‚
â”‚   â”œâ”€â”€ Aufg_board_S2.json  # S2-Aufgaben
â”‚   â”œâ”€â”€ Aufg_board_S3.json  # S3-Aufgaben
â”‚   â”‚
â”‚   â”œâ”€â”€ scenario_config.json  # Ãœbungs-Konfiguration
â”‚   â”œâ”€â”€ group_locations.json  # Feuerwehr-Standorte
â”‚   â”‚
â”‚   â”œâ”€â”€ conf/             # Konfigurationen
â”‚   â”‚   â”œâ”€â”€ vehicles.json      # Fahrzeug-Definitionen
â”‚   â”‚   â”œâ”€â”€ types.json         # Einsatztypen
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ initial/          # Initial-State (Reset)
â”‚   â””â”€â”€ user/             # User-Management
â”‚
â””â”€â”€ utils/                # Helper-Funktionen
```

### Wichtige Daten-Dateien

| Datei | Zweck | GeÃ¤ndert von |
|-------|-------|--------------|
| `board.json` | Einsatzstellen (Kanban) | Frontend + Chatbot |
| `protocol.json` | Protokoll-EintrÃ¤ge | Meldestelle + Chatbot |
| `roles.json` | Aktive/Fehlende Rollen | Frontend + Chatbot |
| `Aufg_board_S2.json` | S2-Aufgaben | S2 + Chatbot |
| `scenario_config.json` | Ãœbungs-Setup | Admin |

**Datenaustausch Chatbot â†” Server:**

```javascript
// chatbot/server/einfo_io.js
export async function readEinfoInputs() {
  const dataDir = path.resolve(__dirname, CONFIG.dataDir);

  // Liest von server/data/
  const board = await readJson(path.join(dataDir, "board.json"));
  const roles = await readJson(path.join(dataDir, "roles.json"));
  const protokoll = await readJson(path.join(dataDir, "protocol.json"));
  // ...

  return { board, roles, protokoll, ... };
}
```

**Pfade:**
- Chatbot liegt in: `/home/user/EINFO/chatbot/`
- Server-Daten: `/home/user/EINFO/server/data/`
- Config sagt: `dataDir: "../../server/data"` (relativ zu `chatbot/server/`)
- Resultat: `/home/user/EINFO/chatbot/server/../../server/data` = `/home/user/EINFO/server/data` âœ…

---

## ðŸ“ PROMPT-TEMPLATES

**Verzeichnis:** `chatbot/server/prompt_templates/`

### Template-Dateien

| Datei | Verwendung |
|-------|------------|
| `start_system_prompt.txt` | Erster Simulationsschritt (Szenario-Initialisierung) |
| `operations_system_prompt.txt` | Normale Simulation (Rollen-Logik, JSON-Schema) |
| `operations_user_prompt.txt` | User-Prompt fÃ¼r Operations (mit Platzhaltern) |
| `chat_system_prompt.txt` | QA-Chat (User-Fragen) |
| `chat_user_prompt.txt` | User-Prompt fÃ¼r Chat |

### Template-Syntax

**Platzhalter:**
```
{{rolesPart}}          â†’ JSON mit active/missing roles
{{compressedBoard}}    â†’ Komprimiertes Board (JSON)
{{knowledgeContext}}   â†’ RAG-Chunks
{{taskSection}}        â†’ Spezielle Anweisungen
{{responseRequests}}   â†’ Meldungen die Antwort brauchen
```

**Beispiel** (`operations_user_prompt.txt`):
```
Aktuelle Rollen:
{{rolesPart}}

Einsatzstellen (kompakt):
{{compressedBoard}}

Knowledge-Kontext:
{{knowledgeContext}}

{{taskSection}}

{{responseRequests}}
```

**Laden & FÃ¼llen** (`prompts.js`):
```javascript
function loadPromptTemplate(fileName) {
  const fullPath = path.join(TEMPLATE_DIR, fileName);
  return fs.readFileSync(fullPath, "utf8").trim();
}

function fillTemplate(template, replacements) {
  return Object.entries(replacements).reduce((acc, [key, value]) => {
    return acc.replaceAll(`{{${key}}}`, value);
  }, template);
}
```

---

## ðŸ”§ WICHTIGE BEFEHLE & SCRIPTS

### Chatbot starten

```bash
cd /home/user/EINFO/chatbot

# Installiere Dependencies (einmalig)
npm install

# Starte Chatbot-Server
npm start
# â†’ http://localhost:3100
```

### Knowledge-Index aufbauen

```bash
cd /home/user/EINFO/chatbot

# VollstÃ¤ndiger Rebuild (5-10 Min)
npm run build-index

# ÃœberprÃ¼fe Status
ls -lh knowledge_index/
# Sollte zeigen:
# - meta.json (~50 KB, 320 Chunks)
# - embeddings.json (~80 MB, Vektoren)
```

### Chatbot Worker (Simulation)

**Separate Prozess** (wird im Hintergrund gestartet):

```bash
cd /home/user/EINFO/server

# Startet Worker, der alle 30 Sek einen Schritt macht
node chatbot_worker.js
```

**Was macht der Worker?**
1. PrÃ¼ft `server/data/roles.json` auf fehlende Rollen
2. Wenn Rollen fehlen â†’ Ruft `/api/sim/step` auf
3. LLM generiert Operations (board, aufgaben, protokoll)
4. Worker schreibt Changes zurÃ¼ck in `server/data/`

**Konfiguration** (`server/chatbot_worker.js`):
```javascript
const CHATBOT_STEP_URL = "http://127.0.0.1:3100/api/sim/step";
const WORKER_INTERVAL_MS = 30000;  // 30 Sekunden
```

---

## ðŸ—‚ï¸ KNOWLEDGE-DATEIEN: Format & Struktur

### UnterstÃ¼tzte Formate

| Format | Extension | Verwendung |
|--------|-----------|------------|
| PDF | `.pdf` | Richtlinien, HandbÃ¼cher |
| Text | `.txt` | Fach-Wissen (Hochwasser, Schnee, etc.) |
| JSON | `.json` | Strukturierte Daten (Rollen, RAG) |

### Knowledge-Typen

#### 1. PDFs (Richtlinien)

```
e31.pdf                  â†’ Info E-31: Stabsarbeit im Feuerwehrdienst
richtlinie.pdf           â†’ FÃ¼hrungsrichtlinien
E5_web.pdf               â†’ Hochwasser-Fachunterlagen
E6_compressed_web.pdf    â†’ Anforderungen im Ereignisfall
```

**Zweck:** Basis-Wissen fÃ¼r Stabsarbeit & Einsatzleitung

#### 2. TXT (Fachwissen)

**Beispiel** (`hochwasser.txt`):
```
Hochwasser - Einsatztaktik und MaÃŸnahmen

GefÃ¤hrdungsbeurteilung:
- Wasserstand und FlieÃŸgeschwindigkeit prÃ¼fen
- Gefahr fÃ¼r Personen und GebÃ¤ude bewerten
- Zufahrtswege und RÃ¼ckzugsmÃ¶glichkeiten sichern

MaÃŸnahmen:
1. Absperren und Warnen
2. Personen evakuieren
3. SandsÃ¤cke und Pumpen einsetzen
...
```

**Zweck:** Spezifisches Fach-Wissen fÃ¼r Chatbot-Antworten

#### 3. JSON (Strukturierte Daten)

**A) Rollen-Definitionen** (`rollen_S2_Lage.json`):
```json
{
  "rolle": "S2 - Lage",
  "kuerzel": "S2",
  "aufgaben": [
    "Lageerfassung und Lagebild erstellen",
    "Lagekarten fÃ¼hren",
    "Lagebeurteilung durchfÃ¼hren",
    "Lageinformationen sammeln und auswerten"
  ],
  "befugnisse": [
    "Anordnung von ErkundungsmaÃŸnahmen",
    "Anforderung von Lageinformationen"
  ],
  "schnittstellen": ["EL", "S3", "S6"]
}
```

**B) RAG-Hazards** (`rag_flood_hazards.json`):
```json
{
  "hazard_type": "flood",
  "scenarios": [
    {
      "id": "flood_01",
      "severity": "high",
      "description": "Starkes Hochwasser mit Ãœberflutungsgefahr",
      "indicators": [
        "Wasserstand Ã¼ber 3m",
        "FlieÃŸgeschwindigkeit > 2m/s"
      ],
      "actions": [
        "Sofortige Evakuierung",
        "Absperren der Gefahrenzone"
      ]
    }
  ]
}
```

**Zweck:** Strukturierte Daten fÃ¼r LLM-Reasoning

---

## ðŸ”¨ KNOWLEDGE-INDEX: Wie funktioniert das?

### Workflow

```
1. Knowledge-Dateien â†’ 2. Text-Extraktion â†’ 3. Chunking â†’ 4. Embeddings â†’ 5. Index
   (22 Dateien)         (PDFâ†’Text)          (~1000 Zeichen)   (Ollama)      (meta.json)
```

### Schritt-fÃ¼r-Schritt

**1. Text-Extraktion** (`index_builder.js`):
```javascript
async function extractText(file) {
  if (file.ext === ".pdf") {
    const buf = await fsPromises.readFile(file.path);
    const data = await pdfParse(buf);  // pdf-parse library
    return data.text || "";
  }
  if (file.ext === ".json") {
    // JSON â†’ Pretty-printed String
    const parsed = JSON.parse(raw);
    return JSON.stringify(parsed, null, 2);
  }
  // TXT â†’ Direkt lesen
  return await fsPromises.readFile(file.path, "utf8");
}
```

**2. Chunking** (`chunk.js`):
```javascript
export function chunkText(text, maxChunkSize = 1000, overlap = 200) {
  const words = text.split(/\s+/);
  const chunks = [];

  for (let i = 0; i < words.length; i += maxChunkSize - overlap) {
    const chunk = words.slice(i, i + maxChunkSize).join(" ");
    chunks.push(chunk);
  }

  return chunks;
}
```

**Warum Overlap?**
- Verhindert, dass wichtige Infos "zwischen" Chunks verloren gehen
- Beispiel: "...Lagebeurteilung durchfÃ¼hren. [CHUNK-GRENZE] S2 erstellt Lagekarten..."
  â†’ Mit Overlap: Beide Chunks enthalten "Lagebeurteilung"

**3. Embeddings** (`embedding.js`):
```javascript
export async function embedText(text) {
  const response = await fetch(`${CONFIG.llmBaseUrl}/api/embeddings`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: CONFIG.llmEmbedModel,  // mxbai-embed-large
      prompt: text
    })
  });

  const data = await response.json();
  return new Float32Array(data.embedding);  // [1024 Zahlen]
}
```

**Was sind Embeddings?**
- Vektoren, die die semantische Bedeutung eines Textes reprÃ¤sentieren
- Ã„hnliche Texte haben Ã¤hnliche Vektoren
- Beispiel:
  ```
  "Hochwasser bekÃ¤mpfen" â†’ [0.12, -0.45, 0.78, ...]
  "Ãœberflutung eindÃ¤mmen" â†’ [0.15, -0.42, 0.80, ...]  (Ã¤hnlich!)
  "Fahrzeug reparieren"  â†’ [-0.23, 0.67, -0.11, ...]  (anders!)
  ```

**4. Index speichern** (`index_builder.js`):
```javascript
// meta.json
await fsPromises.writeFile(metaPath, JSON.stringify({
  dim: 1024,
  files: [
    { name: "e31.pdf", chunks: 107 },
    { name: "hochwasser.txt", chunks: 7 },
    // ...
  ],
  chunks: [
    { id: 0, fileName: "e31.pdf", text: "..." },
    { id: 1, fileName: "e31.pdf", text: "..." },
    // ... 320 Chunks
  ]
}, null, 2));

// embeddings.json
await fsPromises.writeFile(embeddingsPath, JSON.stringify({
  dim: 1024,
  vectors: [
    [0.12, -0.45, ...],  // 1024 Zahlen
    [0.23, 0.56, ...],
    // ... 320 Vektoren
  ]
}));
```

---

## ðŸ” RAG-SUCHE: Wie funktioniert die Knowledge-Retrieval?

### Ablauf bei User-Frage

```
User: "Was sind die Aufgaben von S2?"
   â†“
1. Query-Embedding erstellen
   â†’ [0.34, -0.12, 0.67, ...] (1024-dim)
   â†“
2. Similarity-Search im Index
   â†’ Top-5 Ã¤hnlichste Chunks finden
   â†“
3. Chunks als Context an LLM
   â†’ "Basierend auf: [rollen_S2_Lage.json|0.89] ..."
   â†“
4. LLM generiert Antwort
   â†’ "S2 ist zustÃ¤ndig fÃ¼r: 1. Lageerfassung..."
```

### Code (`rag_vector.js`)

```javascript
export async function getKnowledgeContextVector(query) {
  // 1. Query embedden
  const queryEmbedding = await embedText(query);

  // 2. Similarity berechnen fÃ¼r alle Chunks
  const results = [];
  for (let i = 0; i < vectors.length; i++) {
    const score = cosineSimilarity(queryEmbedding, vectors[i]);

    if (score >= CONFIG.rag.scoreThreshold) {  // >= 0.35
      results.push({
        idx: i,
        score: score,
        text: meta.chunks[i].text,
        fileName: meta.chunks[i].fileName
      });
    }
  }

  // 3. Top-K auswÃ¤hlen (beste 5)
  results.sort((a, b) => b.score - a.score);
  const topK = results.slice(0, CONFIG.rag.topK);

  // 4. Context-String erstellen
  let context = "";
  for (const r of topK) {
    context += `[${r.fileName}|${r.score.toFixed(2)}]\n`;
    context += r.text + "\n\n";
  }

  return context;
}
```

### Cosine-Similarity

```javascript
function cosineSimilarity(a, b) {
  let dot = 0;   // Skalarprodukt
  let na = 0;    // Norm von a
  let nb = 0;    // Norm von b

  for (let i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    na += a[i] * a[i];
    nb += b[i] * b[i];
  }

  return dot / (Math.sqrt(na) * Math.sqrt(nb));
  // Ergebnis: 0.0 (komplett unterschiedlich) bis 1.0 (identisch)
}
```

**Warum Cosine?**
- Misst den Winkel zwischen zwei Vektoren
- UnabhÃ¤ngig von der LÃ¤nge (nur Richtung zÃ¤hlt)
- Perfekt fÃ¼r Embeddings!

---

## ðŸŽ¯ KONFIGURATION FÃœR VERSCHIEDENE USE-CASES

### Use Case 1: Mehr Knowledge-Context

**Problem:** Chatbot findet relevante Infos nicht

**LÃ¶sung** (`config.js`):
```javascript
rag: {
  topK: 8,                    // War: 5
  maxContextChars: 4000,      // War: 2500
  scoreThreshold: 0.30        // War: 0.35 (weniger streng)
}
```

### Use Case 2: Schnellere Antworten

**Problem:** LLM braucht zu lange

**LÃ¶sung:**
```javascript
rag: {
  topK: 3,                    // Weniger Context
  maxContextChars: 1500       // KÃ¼rzer
},

llmSimTimeoutMs: 120000       // 2 Min statt 5 Min
```

### Use Case 3: Andere Models

**Problem:** Llama 3.1 zu langsam, will Mistral verwenden

**LÃ¶sung:**
```javascript
llmChatModel: "mistral:7b",
llmEmbedModel: "nomic-embed-text",

rag: {
  dim: 768                    // nomic = 768-dim, nicht 1024!
}
```

**âš ï¸ WICHTIG:** Nach Model-Wechsel Index neu bauen!
```bash
npm run build-index
```

### Use Case 4: Externe Ollama-Server

**Problem:** Ollama lÃ¤uft auf anderem Server

**LÃ¶sung:**
```bash
export LLM_BASE_URL=http://192.168.1.50:11434
npm start
```

Oder direkt in `config.js`:
```javascript
llmBaseUrl: "http://192.168.1.50:11434",
```

---

## ðŸ”’ WICHTIGE DATEIEN (NICHT LÃ–SCHEN!)

### Kritische Dateien

| Datei/Ordner | Zweck | Bei Verlust |
|--------------|-------|-------------|
| `chatbot/knowledge/` | Source of Truth | Knowledge fehlt! |
| `chatbot/knowledge_index/meta.json` | Chunk-Mapping | Index neu bauen |
| `chatbot/knowledge_index/embeddings.json` | Vektoren | Index neu bauen |
| `chatbot/server/config.js` | Konfiguration | System funktioniert nicht |
| `chatbot/server/prompt_templates/` | Prompts | LLM generiert MÃ¼ll |
| `server/data/` | Datenbank | Alle Daten weg! |

### Kann regeneriert werden

| Datei/Ordner | Regenerieren mit |
|--------------|------------------|
| `chatbot/knowledge_index/` | `npm run build-index` |
| `chatbot/logs/` | Starte Server neu |
| `chatbot/node_modules/` | `npm install` |

---

## ðŸ“Š TYPISCHE DATEIGRÃ–SSEN

```
chatbot/knowledge/
  e31.pdf                      948 KB
  richtlinie.pdf              1700 KB
  E5_web.pdf                  1280 KB
  E6_compressed_web.pdf         50 KB
  *.txt                      2-6 KB je
  *.json                    1-24 KB je

chatbot/knowledge_index/
  meta.json                     50 KB  (320 Chunks)
  embeddings.json            80000 KB  (320 Ã— 1024 Ã— 4 bytes)
  index.json                     1 KB  (leer, legacy)

chatbot/logs/
  chatbot.log                 5-50 KB pro Tag
  LLM.log                   100-500 KB pro Tag
```

---

## â“ FAQ: Konfiguration & Troubleshooting

### Q: Wo stelle ich die Ollama-URL ein?

**A:** `chatbot/server/config.js`, Zeile 17:
```javascript
llmBaseUrl: process.env.LLM_BASE_URL || "http://127.0.0.1:11434",
```

Oder als Umgebungsvariable:
```bash
export LLM_BASE_URL=http://192.168.1.50:11434
```

### Q: Wie Ã¤ndere ich das LLM-Model?

**A:** `config.js`, Zeile 20:
```javascript
llmChatModel: "llama3.1:8b",  // Ã„ndere hier
```

Dann Model laden:
```bash
ollama pull <model-name>
```

### Q: Knowledge-Dateien hinzufÃ¼gen - was muss ich tun?

**A:**
1. Datei in `chatbot/knowledge/` legen (.pdf, .txt, .json)
2. Index neu bauen: `npm run build-index`
3. Fertig!

### Q: Index-Build schlÃ¤gt fehl mit "fetch failed"

**A:** Ollama lÃ¤uft nicht!
```bash
# Check
curl http://localhost:11434/api/tags

# Start
ollama serve
```

### Q: Chatbot findet bestimmte Infos nicht

**A:** Mehrere MÃ¶glichkeiten:
1. **File nicht im Index?** â†’ Check `knowledge_index/meta.json`
2. **Similarity zu niedrig?** â†’ `scoreThreshold` reduzieren (0.30)
3. **Zu wenig Context?** â†’ `topK` erhÃ¶hen (8)

### Q: LLM antwortet zu langsam

**A:**
1. **Kleineres Model:** `llama3.1:8b` â†’ `mistral:7b`
2. **Weniger Context:** `topK: 3`, `maxContextChars: 1500`
3. **GPU nutzen:** Ollama mit CUDA/Metal starten

### Q: Wo finde ich die Logs?

**A:**
- Chatbot: `chatbot/logs/chatbot.log`
- LLM-Calls: `chatbot/logs/LLM.log`
- Verworfene Ops: `chatbot/logs/ops_verworfen.log`

### Q: Config-Ã„nderung wirkt nicht

**A:** Server neu starten!
```bash
# Ctrl+C zum Stoppen
npm start  # Neu starten
```

### Q: Kann ich mehrere Chatbot-Instanzen laufen lassen?

**A:** Ja, aber Port Ã¤ndern:
```bash
export CHATBOT_PORT=3101
npm start
```

---

## ðŸŽ“ LERNRESSOURCEN

### Understanding RAG

- **Was ist RAG?** Retrieval-Augmented Generation = LLM + Knowledge-Suche
- **Warum Embeddings?** Semantische Suche statt Keyword-Matching
- **Chunking?** Lange Texte in verdaubare Happen teilen

### Understanding Prompts

- **System-Prompt:** "Du bist..." (Rolle definieren)
- **User-Prompt:** Konkrete Aufgabe + Daten
- **Templates:** Wiederverwendbare Prompt-Bausteine

### Ollama

- **Docs:** https://ollama.com/library
- **Models:** https://ollama.com/library (llama3.1, mistral, etc.)
- **API:** https://github.com/ollama/ollama/blob/main/docs/api.md

---

## ðŸš€ QUICK-START CHECKLISTE

### Ersteinrichtung

- [ ] Node.js 18+ installiert
- [ ] Ollama installiert & gestartet (`ollama serve`)
- [ ] Models geladen:
  ```bash
  ollama pull llama3.1:8b
  ollama pull mxbai-embed-large
  ```
- [ ] Chatbot-Dependencies:
  ```bash
  cd /home/user/EINFO/chatbot
  npm install
  ```
- [ ] Knowledge-Index bauen:
  ```bash
  npm run build-index
  ```
- [ ] Chatbot starten:
  ```bash
  npm start
  ```
- [ ] Test:
  ```bash
  curl http://localhost:3100/api/admin/knowledge-status
  ```

### Bei Problemen

1. **Logs prÃ¼fen:** `chatbot/logs/chatbot.log`
2. **Ollama lÃ¤uft?** `curl http://localhost:11434/api/tags`
3. **Index vollstÃ¤ndig?** `cat chatbot/knowledge_index/meta.json | grep chunks`
4. **Config korrekt?** `chatbot/server/config.js` Zeile 13-14
5. **Debug-Modus:** `CHATBOT_DEBUG=1 npm start`

---

## ðŸ“ž SUPPORT & WEITERFÃœHRENDE DOCS

- **Haupt-Testbericht:** `/home/user/EINFO/CHATBOT_TEST_REPORT.md`
- **Diese Strukturdoku:** `/home/user/EINFO/PROJEKT_STRUKTUR.md`
- **Ollama Docs:** https://github.com/ollama/ollama
- **LLM Prompting:** https://www.promptingguide.ai/

**Bei Fragen oder Problemen:**
1. Logs prÃ¼fen (`chatbot/logs/`)
2. Debug-Modus aktivieren (`CHATBOT_DEBUG=1`)
3. GitHub Issues: https://github.com/IVOBLA/EINFO/issues

---

*Letzte Aktualisierung: 2025-12-22*
*Version: 1.0 - VollstÃ¤ndige Struktur-Dokumentation*
