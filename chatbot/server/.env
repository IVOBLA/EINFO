# chatbot/server/.env
# GPU-Konfiguration f端r RTX 4070 (8GB VRAM)

# KRITISCH: GPU aktivieren (NICHT OLLAMA_NO_GPU=1 setzen!)
CUDA_VISIBLE_DEVICES=0

# Ollama GPU-Schichten (20-24 f端r 8B Modell auf 8GB VRAM)
OLLAMA_NUM_GPU=22

# Speicher-Optimierung
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_KEEP_ALIVE=30m

# LLM-Konfiguration
LLM_CHAT_MODEL=llama3.1:8b
LLM_EMBED_MODEL=mxbai-embed-large
LLM_TEMP=0.05
LLM_SEED=42

# Timeouts (ms)
LLM_CHAT_TIMEOUT_MS=60000
LLM_SIM_TIMEOUT_MS=300000
LLM_EMBED_TIMEOUT_MS=30000

# RAG-Konfiguration
RAG_DIM=1024
RAG_TOP_K=5
RAG_MAX_CTX=2500
RAG_SCORE_THRESHOLD=0.35

# Debug (0=aus, 1=an)
CHATBOT_DEBUG=0

SIM_WORKER_INTERVAL_MS=60000
SIM_MAX_RETRIES=3
SIM_RETRY_DELAY_MS=5000
MAIN_SERVER_URL=http://localhost:4040

# Multi-Modell Konfiguration
# Verf端gbare Modell-Profile (definiert in config.js)
LLM_MODEL_FAST=llama3.1:8b
LLM_MODEL_BALANCED=einfo-balanced

# Task-spezifische Modellzuordnung (fast | balanced)
LLM_TASK_START=balanced
LLM_TASK_OPS=balanced
LLM_TASK_CHAT=balanced
LLM_TASK_ANALYSIS=balanced
LLM_TASK_DEFAULT=balanced

# Globales Modell-Override (auto | fast | balanced)
# Wenn nicht "auto", wird dieses Modell f端r ALLE Tasks verwendet
LLM_MODEL=auto
